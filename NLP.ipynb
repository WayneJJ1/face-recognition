{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Method 1: Frequency-based Method"
      ],
      "metadata": {
        "id": "co2eCapT8CoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def frequency_based_method(text, top_n=10):\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    word_counts = Counter(words)\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "    return most_common_words\n",
        "\n",
        "text = \"This is an example text. This text is for testing the frequency-based method.\"\n",
        "keywords = frequency_based_method(text, top_n=5)\n",
        "print(keywords)\n"
      ],
      "metadata": {
        "id": "XpxJ37_j8Hsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2: Mean and Variance"
      ],
      "metadata": {
        "id": "d-J4heAa8H9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKqpHQFg7_MI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def mean_variance_method(text, top_n=10):\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    counts = np.array(list(word_counts.values()))\n",
        "    mean = np.mean(counts)\n",
        "    variance = np.var(counts)\n",
        "\n",
        "    important_words = {word: count for word, count in word_counts.items() if count > mean and (count - mean)**2 < variance}\n",
        "\n",
        "    sorted_words = sorted(important_words.items(), key=lambda item: item[1], reverse=True)\n",
        "    return sorted_words[:top_n]\n",
        "\n",
        "keywords = mean_variance_method(text, top_n=5)\n",
        "print(keywords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 3: JKF Justeson Katz Filter"
      ],
      "metadata": {
        "id": "ib6fkn6p8I4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def justeson_katz_filter(text):\n",
        "    jkf_pattern = re.compile(r'\\b(?:\\w+\\s+){0,1}(?P<adj>\\w+)\\s+(?P<noun>\\w+)\\b')\n",
        "    matches = jkf_pattern.findall(text.lower())\n",
        "    term_counts = Counter([\" \".join(match) for match in matches])\n",
        "    return term_counts.most_common()\n",
        "\n",
        "keywords = justeson_katz_filter(text)\n",
        "print(keywords)\n"
      ],
      "metadata": {
        "id": "RxdljvKM8RUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined Method"
      ],
      "metadata": {
        "id": "6yR4_sIm8RjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_keyword_extraction(text, top_n=10):\n",
        "    freq_keywords = frequency_based_method(text, top_n)\n",
        "    mv_keywords = mean_variance_method(text, top_n)\n",
        "    jkf_keywords = justeson_katz_filter(text)[:top_n]\n",
        "\n",
        "    combined_keywords = set([word for word, _ in freq_keywords] +\n",
        "                            [word for word, _ in mv_keywords] +\n",
        "                            [term for term, _ in jkf_keywords])\n",
        "\n",
        "    return combined_keywords\n",
        "\n",
        "combined_keywords = combined_keyword_extraction(text, top_n=10)\n",
        "print(combined_keywords)\n"
      ],
      "metadata": {
        "id": "s8Iuock58U65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z-test"
      ],
      "metadata": {
        "id": "M6qS9XFILK8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# 两个样本数据\n",
        "sample1 = [1, 2, 3, 4, 5]\n",
        "sample2 = [2, 3, 4, 5, 6]\n",
        "\n",
        "# 计算Z值和p值\n",
        "z_stat, p_value = stats.ttest_ind(sample1, sample2)\n",
        "\n",
        "print(\"Z-statistic:\", z_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "Ang6x93xLLIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-test"
      ],
      "metadata": {
        "id": "dC4pWW0wLLU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# 单个样本数据\n",
        "sample = [1, 2, 3, 4, 5]\n",
        "\n",
        "# 假设的均值\n",
        "mu = 3\n",
        "\n",
        "# 计算T值和p值\n",
        "t_stat, p_value = stats.ttest_1samp(sample, mu)\n",
        "\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "w4HjvDmoLLfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "χ²-test"
      ],
      "metadata": {
        "id": "ei0MMyinLLm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# 定义一个二维数组表示观察到的频次\n",
        "observed = [[10, 20], [30, 40]]\n",
        "\n",
        "# 计算χ²值、p值、自由度和期望频次\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(observed)\n",
        "\n",
        "print(\"χ²-statistic:\", chi2_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "1f3p-sUoLf2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}